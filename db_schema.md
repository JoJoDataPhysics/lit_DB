# ğŸ—„ï¸ Database Schema Documentation

## ğŸ“‹ Overview

The **lit_DB** application uses SQLite to store PDF file metadata, analysis results, and extracted topics with keywords. The schema is designed for **hash-based deduplication**, **efficient querying**, and **maintaining analysis history** across different AI models.

### ğŸ¯ **Key Features**
- **ğŸ” Deduplication**: Prevents reprocessing of identical file+model combinations
- **ğŸ“Š Rich Metadata**: Comprehensive PDF metadata extraction and storage
- **ğŸ”— Referential Integrity**: Proper foreign key relationships
- **âš¡ Performance**: Optimized indexes for fast lookups
- **ğŸ§  AI Model Tracking**: Support for multiple LLM models per document

---

## ğŸ—ï¸ Entity Relationship Diagram

```mermaid
%%{init: {
  "theme": "base",
  "themeVariables": {
    "primaryColor": "#e3f2fd",
    "primaryTextColor": "#0d47a1",
    "primaryBorderColor": "#1976d2",
    "lineColor": "#424242",
    "secondaryColor": "#f3e5f5",
    "tertiaryColor": "#e8f5e8",
    "background": "#ffffff",
    "mainBkg": "#ffffff",
    "secondBkg": "#f5f5f5"
  }
}}%%

erDiagram
    %% Entity Definitions with Enhanced Styling
    files {
        integer id PK "ğŸ”‘ Primary Key"
        text file_path UK "ğŸ“ Full filesystem path"
        text filename "ğŸ“„ PDF filename only"
        text file_hash UK "ğŸ” SHA-256 content hash"
        integer file_size "ğŸ“ File size in bytes"
        integer page_count "ğŸ“ƒ Number of PDF pages"
        integer word_count "ğŸ“ Total extracted words"
        text author "ğŸ‘¤ PDF author metadata"
        text title "ğŸ“– PDF title metadata"
        text subject "ğŸ“‹ PDF subject metadata"
        text creator "ğŸ› ï¸ PDF creator application"
        text producer "âš™ï¸ PDF producer application"
        text creation_date "ğŸ“… PDF creation timestamp"
        text modification_date "ğŸ“… File modification date"
        datetime first_analyzed "ğŸ• First analysis time"
        datetime last_analyzed "ğŸ• Most recent analysis"
    }
    
    analysis_results {
        integer id PK "ğŸ”‘ Primary Key"
        integer file_id FK "ğŸ”— Foreign Key â†’ files"
        text analysis_hash UK "ğŸ” SHA-256 dedup hash"
        text model_name "ğŸ¤– AI model identifier"
        real confidence_score "ğŸ“Š Analysis confidence 0-1"
        datetime analysis_timestamp "ğŸ• Processing timestamp"
    }
    
    topics {
        integer id PK "ğŸ”‘ Primary Key"
        integer analysis_id FK "ğŸ”— Foreign Key â†’ analysis_results"
        text topic "ğŸ·ï¸ Extracted topic name"
        real confidence_score "ğŸ“Š Topic confidence 0-1"
        text keywords "ğŸ”— JSON array of keywords"
    }
    
    %% Relationships with Enhanced Labels
    files ||--o{ analysis_results : "ğŸ“Š generates"
    analysis_results ||--o{ topics : "ğŸ·ï¸ extracts"
    
    %% Enhanced Styling Classes
    files {
        string backgroundColor "#e3f2fd"
        string borderColor "#1976d2"
        string textColor "#0d47a1"
    }
    
    analysis_results {
        string backgroundColor "#f3e5f5"
        string borderColor "#7b1fa2"
        string textColor "#4a148c"
    }
    
    topics {
        string backgroundColor "#e8f5e8"
        string borderColor "#388e3c"
        string textColor "#1b5e20"
    }
```

---

## ğŸ“Š Table Specifications

### ğŸ“ **`files`** - PDF File Registry
> ğŸ¯ **Purpose**: Central registry of all PDF files with comprehensive metadata and processing history

#### ğŸ“‹ **Column Details**
| Column | Type | Constraints | Description |
|--------|------|-------------|-------------|
| ğŸ”‘ `id` | `INTEGER` | `PRIMARY KEY` | Unique file identifier |
| ğŸ“ `file_path` | `TEXT` | `NOT NULL` | Full filesystem path to PDF |
| ğŸ“„ `filename` | `TEXT` | `NOT NULL` | PDF filename only |
| ğŸ” `file_hash` | `TEXT` | `NOT NULL` | SHA-256 hash of file content |
| ğŸ“ `file_size` | `INTEGER` | - | File size in bytes |
| ğŸ“ƒ `page_count` | `INTEGER` | - | Number of pages in PDF |
| ğŸ“ `word_count` | `INTEGER` | - | Total word count from text extraction |
| ğŸ‘¤ `author` | `TEXT` | - | PDF author from metadata |
| ğŸ“– `title` | `TEXT` | - | PDF title from metadata |
| ğŸ“‹ `subject` | `TEXT` | - | PDF subject from metadata |
| ğŸ› ï¸ `creator` | `TEXT` | - | PDF creator application |
| âš™ï¸ `producer` | `TEXT` | - | PDF producer application |
| ğŸ“… `creation_date` | `TEXT` | - | PDF creation date from metadata |
| ğŸ“… `modification_date` | `TEXT` | - | File modification date |
| ğŸ• `first_analyzed` | `DATETIME` | `DEFAULT CURRENT_TIMESTAMP` | First analysis timestamp |
| ğŸ• `last_analyzed` | `DATETIME` | `DEFAULT CURRENT_TIMESTAMP` | Most recent analysis timestamp |

#### ğŸ”’ **Constraints & Indexes**
- ğŸš« **Unique Constraint**: `UNIQUE(file_path, file_hash)` - Prevents duplicate entries
- âš¡ **Performance Indexes**:
  - `idx_files_hash` on `file_hash` - Lightning-fast hash lookups
  - `idx_files_path` on `file_path` - Quick path-based searches

---

### ğŸ¤– **`analysis_results`** - AI Analysis Registry
> ğŸ¯ **Purpose**: Links PDF files to AI model analysis results with deduplication

#### ğŸ“‹ **Column Details**
| Column | Type | Constraints | Description |
|--------|------|-------------|-------------|
| ğŸ”‘ `id` | `INTEGER` | `PRIMARY KEY` | Unique analysis identifier |
| ğŸ”— `file_id` | `INTEGER` | `FOREIGN KEY` | References `files(id)` |
| ğŸ” `analysis_hash` | `TEXT` | `UNIQUE, NOT NULL` | SHA-256 of `file_hash + model_name` |
| ğŸ¤– `model_name` | `TEXT` | `NOT NULL` | AI model used (e.g., "mistral:7b") |
| ğŸ“Š `confidence_score` | `REAL` | `0.0-1.0` | Overall analysis confidence |
| ğŸ• `analysis_timestamp` | `DATETIME` | `DEFAULT CURRENT_TIMESTAMP` | When analysis was performed |

#### ğŸ”’ **Constraints & Indexes**
- ğŸ”— **Foreign Key**: `FOREIGN KEY (file_id) REFERENCES files (id)` - Referential integrity
- ğŸš« **Unique Constraint**: `UNIQUE(analysis_hash)` - Prevents duplicate analyses
- âš¡ **Performance Index**: `idx_analysis_hash` - Ultra-fast deduplication lookups

---

### ğŸ·ï¸ **`topics`** - Extracted Topics & Keywords
> ğŸ¯ **Purpose**: Stores AI-extracted topics and keywords for each analysis

#### ğŸ“‹ **Column Details**
| Column | Type | Constraints | Description |
|--------|------|-------------|-------------|
| ğŸ”‘ `id` | `INTEGER` | `PRIMARY KEY` | Unique topic identifier |
| ğŸ”— `analysis_id` | `INTEGER` | `FOREIGN KEY` | References `analysis_results(id)` |
| ğŸ·ï¸ `topic` | `TEXT` | `NOT NULL` | Extracted topic name |
| ğŸ“Š `confidence_score` | `REAL` | `0.0-1.0` | Topic confidence score |
| ğŸ”— `keywords` | `TEXT` | `JSON` | JSON array of related keywords |

#### ğŸ”’ **Constraints**
- ğŸ”— **Foreign Key**: `FOREIGN KEY (analysis_id) REFERENCES analysis_results (id)` - Links to analysis

---

## ğŸ”„ Deduplication Strategy

### ğŸ§  **Smart Two-Level Hash System**

The database implements an intelligent deduplication mechanism using a **dual-hash architecture**:

```mermaid
graph LR
    subgraph "ğŸ“„ File Level"
        A[PDF Content] --> B[SHA-256 Hash]
        B --> C[ğŸ” file_hash]
    end
    
    subgraph "ğŸ¤– Analysis Level"
        C --> D[file_hash + model_name]
        D --> E[SHA-256 Hash]
        E --> F[ğŸ” analysis_hash]
    end
    
    style A fill:#e3f2fd
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style E fill:#f3e5f5
    style F fill:#e8f5e8
```

### ğŸ¯ **Deduplication Logic**

| Scenario | File Hash | Model | Result | Action |
|----------|-----------|-------|---------|---------|
| ğŸ“„ **Same file, same model** | âœ… Match | âœ… Match | ğŸš« **SKIP** | Return cached result |
| ğŸ“„ **Same file, different model** | âœ… Match | âŒ Different | âœ… **ANALYZE** | New analysis entry |
| ğŸ“„ **Different file, same model** | âŒ Different | âœ… Match | âœ… **ANALYZE** | New file + analysis |
| ğŸ“„ **File content changed** | âŒ Different | âœ… Match | âœ… **ANALYZE** | New analysis (even same filename) |

### ğŸš€ **Performance Benefits**
- âš¡ **Instant Lookups**: O(1) hash-based deduplication checks
- ğŸ’¾ **Storage Efficiency**: No redundant analysis data
- ğŸ”„ **Model Flexibility**: Same file can be analyzed with multiple AI models
- ğŸ“Š **History Tracking**: Maintains complete analysis timeline

---

## ğŸ“ Query Patterns & Examples

### ğŸ” **Deduplication Check**
```sql
-- Fast existence check for analysis
SELECT 1 FROM analysis_results 
WHERE analysis_hash = ? 
LIMIT 1;
```

### ğŸ“Š **Complete Analysis Retrieval**
```sql
-- Get full analysis with file metadata and topics
SELECT 
    ar.id, ar.analysis_hash, ar.model_name, ar.confidence_score,
    f.filename, f.file_path, f.author, f.title, f.page_count,
    GROUP_CONCAT(t.topic) as topics,
    GROUP_CONCAT(t.keywords) as all_keywords
FROM analysis_results ar
JOIN files f ON ar.file_id = f.id
LEFT JOIN topics t ON ar.id = t.analysis_id
WHERE ar.analysis_hash = ?
GROUP BY ar.id;
```

### ğŸ·ï¸ **Topic & Keywords Extraction**
```sql
-- Get all topics with keywords for an analysis
SELECT 
    topic,
    confidence_score,
    JSON_EXTRACT(keywords, '$') as keyword_array
FROM topics 
WHERE analysis_id = ?
ORDER BY confidence_score DESC;
```

### ğŸ“ˆ **Analytics & Reporting**
```sql
-- Recent analyses with comprehensive metadata
SELECT 
    f.filename,
    f.author,
    f.title,
    f.page_count,
    f.word_count,
    COUNT(DISTINCT ar.id) as analysis_count,
    GROUP_CONCAT(DISTINCT ar.model_name) as models_used,
    MAX(ar.analysis_timestamp) as last_analysis,
    COUNT(DISTINCT t.id) as total_topics
FROM files f
LEFT JOIN analysis_results ar ON f.id = ar.file_id
LEFT JOIN topics t ON ar.id = t.analysis_id
GROUP BY f.id
ORDER BY last_analysis DESC
LIMIT ?;
```

### ğŸ“Š **Database Statistics**
```sql
-- Comprehensive database metrics
SELECT 
    (SELECT COUNT(*) FROM files) as total_files,
    (SELECT COUNT(*) FROM analysis_results) as total_analyses,
    (SELECT COUNT(*) FROM topics) as total_topics,
    (SELECT COUNT(DISTINCT model_name) FROM analysis_results) as unique_models,
    (SELECT AVG(confidence_score) FROM analysis_results) as avg_confidence;
```

---

## ğŸ› ï¸ Database Operations

### ğŸ”§ **DatabaseManager Class Methods**

#### ğŸ” **Deduplication Methods**
| Method | Purpose | Returns |
|--------|---------|---------|
| `file_already_analyzed(analysis_hash)` | âš¡ Ultra-fast existence check | `bool` |
| `get_existing_analysis(analysis_hash)` | ğŸ“Š Retrieve complete cached result | `AnalysisResult \| None` |
| `_create_analysis_hash(file_hash, model_name)` | ğŸ” Generate deduplication hash | `str` |

#### ğŸ’¾ **Storage Methods**
| Method | Purpose | Returns |
|--------|---------|---------|
| `save_analysis_result(result, metadata, file_hash)` | ğŸ’¾ Store new analysis with topics | `int` (analysis_id) |
| `_upsert_file(conn, result, metadata, file_hash)` | ğŸ“ Insert/update file record | `int` (file_id) |

#### ğŸ“Š **Query Methods**  
| Method | Purpose | Returns |
|--------|---------|---------|
| `get_database_stats()` | ğŸ“ˆ Database metrics & analytics | `Dict[str, Any]` |
| `get_analyzed_files(limit)` | ğŸ“‹ Recent files with metadata | `List[Dict[str, Any]]` |

#### âš ï¸ **Management Methods**
| Method | Purpose | âš ï¸ Warning |
|--------|---------|------------|
| `reset_database()` | ğŸ—‘ï¸ Drop and recreate all tables | **DESTRUCTIVE** - Deletes all data! |

---

## âš™ï¸ Configuration

### ğŸ“‹ **Database Settings** (`config.yaml`)

```yaml
database:
  path: "./data/lit_db.sqlite"      # ğŸ“ SQLite database file location
  enable_persistence: true          # ğŸ’¾ Enable database storage (vs JSON-only)
  backup_json: true                 # ğŸ“„ Also save JSON files as backup
```

---

## ğŸ–¥ï¸ CLI Commands

### ğŸ“Š **Database Management Commands**

| Command | Purpose | Options |
|---------|---------|---------|
| `python main.py db-status` | ğŸ“ˆ Show database statistics & metrics | - |
| `python main.py list-analyzed` | ğŸ“‹ List recently analyzed files | `--limit N` (default: 20) |
| `python main.py drop-db` | ğŸ—‘ï¸ Reset database (âš ï¸ destructive!) | Confirmation prompt |

### ğŸ’¡ **Usage Examples**
```bash
# View database overview
python main.py db-status

# Show last 50 analyzed files with metadata
python main.py list-analyzed --limit 50

# Reset database (requires confirmation)
python main.py drop-db
```

---

## ğŸ”„ Data Flow Architecture

```mermaid
graph TB
    A[ğŸ“– PDF Analysis Request] --> B{ğŸ” Check analysis_hash in DB}
    B -->|âœ… Exists| C[ğŸ“Š Return Cached Result]
    B -->|âŒ New| D[ğŸ“„ Process PDF]
    D --> E[ğŸ§  Extract Metadata]
    E --> F[ğŸ¤– Run LLM Analysis]
    F --> G[ğŸ’¾ Store in Database]
    G --> H[ğŸ“„ Optional JSON Backup]
    
    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#e8f5e8
    style D fill:#f3e5f5
    style E fill:#f3e5f5
    style F fill:#ffebee
    style G fill:#e8f5e8
    style H fill:#fff9c4
```

---

## âš¡ Performance Optimizations

### ğŸš€ **Lightning-Fast Indexes**
| Index | Table | Purpose | Performance |
|-------|-------|---------|------------|
| `idx_files_hash` | `files` | Hash-based lookups | **O(1)** - Instant |
| `idx_analysis_hash` | `analysis_results` | Deduplication checks | **O(1)** - Instant |
| `idx_files_path` | `files` | Path-based searches | **O(log n)** - Very fast |

### ğŸ’¾ **Storage Efficiency**
- ğŸ—‚ï¸ **Normalized Schema**: Eliminates data duplication across tables
- ğŸ“ **JSON Storage**: Keywords stored as TEXT (SQLite compatible)
- ğŸ”„ **Metadata Caching**: Avoids repeated PDF parsing operations
- ğŸ“Š **Referential Integrity**: Foreign keys maintain data consistency

### ğŸ¯ **Deduplication Benefits**
- âš¡ **Zero Reprocessing**: Identical file+model combinations skipped instantly
- ğŸš€ **Database Speed**: Hash lookups faster than JSON file scanning
- ğŸ¤– **Multi-Model Support**: Same file analyzed with different AI models
- ğŸ“ˆ **Scalability**: Performance remains constant as database grows

---

## ğŸ”§ Schema Evolution Guide

### ğŸ“‹ **Adding New Features**

1. **ğŸ”„ Update Database Schema**
   ```python
   # In src/database_manager.py
   def _init_database(self):
       # Add your new columns here
   ```

2. **ğŸ“Š Update Data Models**
   ```python  
   # In src/models.py
   class AnalysisResult(BaseModel):
       # Add new fields here
   ```

3. **ğŸ§ª Test Changes**
   ```bash
   python test_setup.py  # Verify database functionality
   ```

4. **ğŸ“ Update Documentation**
   - Update this `db_schema.md` file
   - Add migration notes if needed

### âš ï¸ **Migration Safety**
- Uses `CREATE TABLE IF NOT EXISTS` for safe initialization
- Existing data preserved during updates
- Always backup database before schema changes
- Test migrations on development data first

---

> ğŸ“š **Need Help?** Check the [DatabaseManager source code](src/database_manager.py) for implementation details or run `python main.py db-status` to verify your database setup.